# merge_three_tables_verbose.py
import pandas as pd

pd.set_option("display.width", 180)
pd.set_option("display.max_columns", 200)

# ============== å°å·¥å…·ï¼šç»Ÿä¸€æ‰“å°ä¿¡æ¯ ==============
def show_info(df, name, key_cols=("symbol", "date"), head_n=5):
    print(f"\n{'='*30} [{name}] åŸºæœ¬ä¿¡æ¯ {'='*30}")
    print(f"shape: {df.shape}")
    print("columns:", list(df.columns))
    print("\ndtypes:\n", df.dtypes)
    if head_n:
        print(f"\n.head({head_n}):\n", df.head(head_n))
    # å…³é”®åˆ—å­˜åœ¨æ€§ä¸ç¼ºå¤±
    for c in key_cols:
        print(f"- has column '{c}':", c in df.columns)
        if c in df.columns:
            miss = df[c].isna().sum()
            print(f"  missing {c}: {miss}")
    # å”¯ä¸€ symbol æ•°ã€æ—¥æœŸèŒƒå›´
    if "symbol" in df.columns:
        print("unique symbols:", df["symbol"].nunique())
        print("top symbols (value_counts head 10):\n", df["symbol"].value_counts().head(10))
    if "date" in df.columns and pd.api.types.is_datetime64_any_dtype(df["date"]):
        print("date min/max:", df["date"].min(), " ~ ", df["date"].max())

def dedup_log(df, subset, name):
    before = len(df)
    df2 = df.drop_duplicates(subset=subset, keep="last")
    after = len(df2)
    if after != before:
        print(f"[{name}] å»é‡: {before} -> {after}ï¼ˆæŒ‰ {subset}ï¼‰")
    else:
        print(f"[{name}] æ— éœ€å»é‡ï¼ˆæŒ‰ {subset}ï¼‰")
    return df2

# ============== 1) è¯»å– ==============
df1 = pd.read_csv("/Users/elviral/codeproject/capstone/data/merge1_onchain_data.csv")
df2 = pd.read_csv("/Users/elviral/codeproject/capstone/data/merge2_cryptocompare_ohlcv.csv")
df3 = pd.read_csv("/Users/elviral/codeproject/capstone/data/merge3_market_cap_data.csv")

print("âœ… å·²è¯»å–ä¸‰ä¸ªCSVæ–‡ä»¶ã€‚")

# ============== 2) åˆ—åæ ‡å‡†åŒ– ==============
def normalize_columns(df):
    # å…¨å°å†™ã€å»é¦–å°¾ç©ºæ ¼ã€ç©ºæ ¼ä¸ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    cols = (
        df.columns
          .str.strip()
          .str.lower()
          .str.replace(r"[ \t/\\]+", "_", regex=True)
          .str.replace(r"[^0-9a-zA-Z_]+", "", regex=True)
    )
    df.columns = cols
    return df

df1 = normalize_columns(df1)
df2 = normalize_columns(df2)
df3 = normalize_columns(df3)

# df3 æœ‰åˆ—å 'data' -> 'date'
if "data" in df3.columns and "date" not in df3.columns:
    df3 = df3.rename(columns={"data": "date"})

# ============== 3) ç»Ÿä¸€ Symbol å¤§å°å†™ã€Date ç±»å‹ ==============
for df, name in [(df1, "onchain"), (df2, "ohlcv"), (df3, "mcap")]:
    if "symbol" in df.columns:
        df["symbol"] = df["symbol"].astype(str).str.strip().str.upper()
    else:
        raise ValueError(f"[{name}] ç¼ºå°‘ symbol åˆ—ã€‚")

    if "date" in df.columns:
        # å¸¸è§æ ¼å¼ä¾‹å¦‚: 2020/1/1ï¼›ç»Ÿä¸€è½¬æ¢å¹¶å»æ‰æ—¶é—´éƒ¨åˆ†
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df["date"] = df["date"].dt.normalize()
    else:
        raise ValueError(f"[{name}] ç¼ºå°‘ date åˆ—ï¼ˆæˆ–éœ€æŠŠ data é‡å‘½åä¸º dateï¼‰ã€‚")

show_info(df1, "df1_onchain(ç»Ÿä¸€åˆ—åå)")
show_info(df2, "df2_ohlcv(ç»Ÿä¸€åˆ—åå)")
show_info(df3, "df3_mcap(ç»Ÿä¸€åˆ—åå)")

# ============== 4) å…³é”®åˆ—å»é‡ï¼ˆæŒ‰ symbol+dateï¼‰===============
df1 = dedup_log(df1, subset=["symbol", "date"], name="df1_onchain")
df2 = dedup_log(df2, subset=["symbol", "date"], name="df2_ohlcv")
df3 = dedup_log(df3, subset=["symbol", "date"], name="df3_mcap")

# ============== 5) åˆå¹¶ï¼ˆå¤–è¿æ¥ï¼Œä¿ç•™å…¨éƒ¨è®°å½•ï¼‰=============
print("\nğŸ§© å¼€å§‹åˆå¹¶ df1 ä¸ df2ï¼ˆouter on ['symbol', 'date']ï¼‰...")
merged_12 = pd.merge(df1, df2, on=["symbol", "date"], how="outer", suffixes=("", "_ohlcvdup"))
show_info(merged_12, "merged_12")

# æ£€æŸ¥æ½œåœ¨é‡å¤å‘½åï¼ˆå¦‚æœä¸¤è¾¹éƒ½æœ‰åŒååˆ—ï¼Œpandasä¼šåŠ åç¼€ï¼‰
dup_cols = [c for c in merged_12.columns if c.endswith("_ohlcvdup")]
if dup_cols:
    print("âš ï¸ å‘ç°åŒååˆ—é‡å¤ï¼ˆæ¥è‡ªç¬¬äºŒå¼ è¡¨ï¼‰ï¼Œä¿ç•™åŸåˆ—ï¼Œåˆ é™¤å¸¦åç¼€åˆ—ï¼š", dup_cols)
    merged_12 = merged_12.drop(columns=dup_cols)

print("\nğŸ§© å°†å¸‚å€¼æ•°æ®å¹¶å…¥ï¼ˆouter on ['symbol', 'date']ï¼‰...")
cols_keep_mcap = ["date", "symbol"]
for c in ["market_cap_usd", "rank", "name"]:
    if c in df3.columns:
        cols_keep_mcap.append(c)
    else:
        print(f"âš ï¸ df3 ä¸å«åˆ—: {c}")

merged = pd.merge(merged_12, df3[cols_keep_mcap], on=["symbol", "date"], how="outer")
show_info(merged, "merged(all three)")

# ============== 6) åŸºç¡€è´¨é‡æ£€æŸ¥ä¸ç©ºå€¼ç»Ÿè®¡ ==============
print("\nğŸ“Š å…³é”®æ•°å€¼åˆ—ç©ºå€¼ç»Ÿè®¡ï¼ˆåªæŒ‘é€‰å­˜åœ¨çš„åˆ—ï¼‰")
num_candidates = [
    "open","high","low","close","volumefrom","volumeto",
    "market_cap_usd",
    "active_addre","average_transaction_value","new_addres","transaction_count","time"
]
present = [c for c in num_candidates if c in merged.columns]
print(merged[present].isna().sum().sort_values(ascending=False))

# ============== 7) æ’åºä¸å¯¼å‡º ==============
merged = merged.sort_values(["symbol", "date"])
merged.to_csv("merged_full_dataset.csv", index=False)
print("\nâœ… å·²å¯¼å‡ºï¼šmerged_full_dataset.csvï¼ˆouter join å…¨é‡ä¿ç•™ï¼‰")

# ============== 8) å¯é€‰ï¼šä»…ä¿ç•™ä¸‰è¡¨éƒ½å‡ºç°è¿‡çš„ä¸¥æ ¼äº¤é›†ï¼ˆinner joinï¼‰ ==============
# è‹¥éœ€è¦â€œä¸¥æ ¼äº¤é›†â€ï¼Œå¯è¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
inner = pd.merge(
    pd.merge(df1[["symbol","date"]], df2[["symbol","date"]], on=["symbol","date"], how="inner"),
    df3[["symbol","date"]], on=["symbol","date"], how="inner"
)
merged_inner = pd.merge(merged, inner, on=["symbol","date"], how="inner")
merged_inner = merged_inner.sort_values(["symbol","date"])
merged_inner.to_csv("merged_strict_inner.csv", index=False)
print("âœ… å·²å¯¼å‡ºï¼šmerged_strict_inner.csvï¼ˆä»…ä¸‰è¡¨éƒ½åŒæ—¶å­˜åœ¨çš„è®°å½•ï¼‰")

# ============== 9) ï¼ˆå¯é€‰ï¼‰æ—¶é—´èŒƒå›´ç­›é€‰ç¤ºä¾‹ ==============
# å¦‚æœä½ è¦é™åˆ¶åˆ°å›ºå®šæ—¶é—´çª—ï¼ˆä¾‹å¦‚ 2020-01-01 è‡³ 2024-12-30ï¼‰ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
# start, end = pd.Timestamp("2020-01-01"), pd.Timestamp("2024-12-30")
# win = merged[(merged["date"] >= start) & (merged["date"] <= end)].copy()
# win.to_csv("merged_full_dataset_20200101_20241230.csv", index=False)
# print("âœ… å·²å¯¼å‡ºï¼šmerged_full_dataset_20200101_20241230.csvï¼ˆæ—¶é—´èŒƒå›´ç­›é€‰åï¼‰")
